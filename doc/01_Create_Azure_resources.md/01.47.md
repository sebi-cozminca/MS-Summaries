---
title: '47: TechLab: LLM App - Prompt Injection, Jailbreak and Protection [Parent]'
layout: default
nav_order: 47
parent: 'Lab summaries'
--- 

# Lab Metadata & Summary

**ID** 196086  
**Number:** TechConnect LAB417  
**Name:** TechLab: LLM App - Prompt Injection, Jailbreak and Protection [Parent]  
**CloudSubscriptionPoolName:** Microsoft CSU CSS - Recycling (Prod)  
**AllowSave:** True  
**CloudCredentialPoolAssignments:** NA  
**Additional licenses:** NA【144†source】  

---

## Exercise Summary

### Exercise 0: Initialize environment
- Installed Python 3.9 and LangChain dependencies.  
- Verified Prompt Flow installation in VS Code.  

### Exercise 1: First Prompt Flow App
- Created new Prompt Flow project in VS Code.  
- Modified prompt template and executed simple app.  
- Validated outputs using Prompt Flow CLI.  

### Exercise 2: Challenge – Black Snow
- Explored model manipulation through adversarial prompts.  
- Observed risks of unexpected outputs.  

### Exercise 3: Prompt Injection
- Implemented restrictive prompt template.  
- Tested bypass attempts with manipulated input.  
- Learned techniques for detecting injections.  

### Exercise 4: Jailbreak scenarios
- Crafted jailbreak prompts to override model restrictions.  
- Compared success rates of different approaches.  

### Exercise 5: Protection strategies
- Applied guardrails with Prompt Flow templates.  
- Integrated defensive patterns to block unsafe prompts.  
