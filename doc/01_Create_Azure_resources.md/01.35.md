---
title: '35: Fine-Tune End-to-End Distillation Models with Azure AI (Catalog Dev)'
layout: default
nav_order: 35
parent: 'Lab summaries'
--- 

# Lab Metadata & Summary

**ID** 191951  
**Number:** LAB329  
**Name:** Fine-Tune End-to-End Distillation Models with Azure AI (Catalog Dev)  
**CloudSubscriptionPoolName:** Microsoft Event Subscription (CSS)  
**AllowSave:** True  
**CloudCredentialPoolAssignments:** NA  
**Additional licenses:** NA  

---

## Exercise Summary
### Exercise 0: Setup Azure AI Foundry project
- **Goal:** Provision workspace and resources for distillation training.  
- **Actions:** Create AI Foundry project, dataset assets, and compute clusters.  
- **Validation:** Resources available; datasets visible in portal.  

### Exercise 1: Prepare datasets
- **Goal:** Stage teacher/student datasets.  
- **Actions:** Upload labeled data to blob storage, register in AI Foundry.  
- **Validation:** Dataset preview shows correct schema.  

### Exercise 2: Configure distillation job
- **Goal:** Train smaller student model from larger teacher.  
- **Actions:** Set job parameters (epochs, learning rate, model sizes); submit via SDK/CLI.  
- **Validation:** Job status succeeds; logs show knowledge distillation steps.  

### Exercise 3: Evaluate and deploy
- **Goal:** Validate accuracy and latency improvements.  
- **Actions:** Run evaluation notebook; compare teacher vs student metrics. Deploy student to endpoint.  
- **Validation:** Endpoint live; metrics within target thresholds.  
