---
title: '33: Evaluate and improve the quality and safety of your AI applications (Catalog Dev)` 
layout: default
nav_order: 33
parent: 'Lab summaries'
--- 

# Lab Metadata & Summary

**ID** 191954  
**Number:** LAB334  
**Name:** Evaluate and improve the quality and safety of your AI applications (Catalog Dev)  
**CloudSubscriptionPoolName:** Microsoft Event Subscription (CSS)  
**AllowSave:** True  
**CloudCredentialPoolAssignments:** NA  
**Additional licenses:** NA  

---

## Exercise Summary
### Exercise 0: Setup & datasets
- **Goal:** Prepare evaluation environment and sample tasks.
- **Actions:** Fork repo / open Codespaces, configure `.env`, and load starter datasets.
- **Validation:** Notebooks run; data preview looks correct.

### Exercise 1: Built‑in quality & safety evaluators
- **Goal:** Run automated metrics to baseline your app.
- **Actions:** Execute notebooks to compute groundedness, similarity, harmful content, jailbreak susceptibility.
- **Validation:** Metrics exported to CSV; failing items logged for review.

### Exercise 2: Custom evaluators & simulators
- **Goal:** Extend with task‑specific checks.
- **Actions:** Implement custom scoring functions and generate synthetic test sets with simulators.
- **Validation:** New metrics appear in report; thresholds defined.

### Exercise 3: Manual triage & observability
- **Goal:** Close the loop on flagged failures.
- **Actions:** Use portal to inspect traces, annotate failures, and re‑run targeted tests.
- **Validation:** Before/after scores improve; residual risks documented.
