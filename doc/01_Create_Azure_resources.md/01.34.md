---
title: '34: Evaluate Reasoning Models for Your Generative AI Solutions (Catalog Dev)` 
layout: default
nav_order: 34
parent: 'Lab summaries'
--- 

# Lab Metadata & Summary

**ID** 191953  
**Number:** LAB333  
**Name:** Evaluate Reasoning Models for Your Generative AI Solutions (Catalog Dev)  
**CloudSubscriptionPoolName:** Microsoft Event Subscription (CSS)  
**AllowSave:** True  
**CloudCredentialPoolAssignments:** NA  
**Additional licenses:** NA  

---

## Exercise Summary
### Exercise 0: Reasoning model concepts
- **Goal:** Understand what makes reasoning models different.
- **Actions:** Review chain‑of‑thought, tool‑use, and self‑consistency patterns with examples.
- **Validation:** Learner can articulate when to prefer a reasoning model.

### Exercise 1: Explore models in Azure AI Foundry
- **Goal:** Try multiple reasoning models and prompts.
- **Actions:** Deploy models, run notebook prompts across tasks (math, coding, planning), and capture traces.
- **Validation:** Notebook logs show stepwise reasoning and success rates.

### Exercise 2: Evaluate & compare
- **Goal:** Establish a fair comparison across models.
- **Actions:** Use evaluation notebooks to compute quality and latency; apply cost normalization.
- **Validation:** Table ranks models by fitness for purpose; recommendation recorded.

### Exercise 3: Patterns & best practices
- **Goal:** Apply prompting and guardrails.
- **Actions:** Add verification, constrained decoding, and tool‑use scaffolds; retest.
- **Validation:** Error rate drops on targeted tasks; guidance captured for reuse.
